\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{verbatim}

\title{KV-Sim: KV-Cache-Aware LLM Serving Simulator (Project Statement)}
\author{}
\date{}

\begin{document}
\maketitle

\section*{1) Overarching overview}

\subsection*{What you're building}
A simulator of an LLM inference serving system that handles many requests on a GPU with limited VRAM. The simulator models:
\begin{itemize}[leftmargin=*]
  \item request arrivals + queueing
  \item prefill vs decode
  \item KV-cache growth over time
  \item GPU capacity constraints (VRAM + concurrency + token throughput)
  \item serving policies (admission, batching/scheduling, eviction/preemption)
\end{itemize}

It does not run a real model. Instead, it predicts system behavior using a resource/latency model and generates reproducible artifacts (summary metrics, time series, and coarse event traces).

\subsection*{Why it matters}
In production, the bottleneck is often KV cache VRAM, not compute. The simulator helps answer:
\begin{itemize}[leftmargin=*]
  \item ``What happens to p95 latency if we increase VRAM?''
  \item ``Which policy avoids OOM with minimal rejects?''
  \item ``How sensitive are results to long-context tail workloads?''
  \item ``What scheduling strategy maximizes throughput without killing fairness?''
\end{itemize}

\section*{2) What the simulator is simulating}

\subsection*{Core objects}

\subsubsection*{Request}
Represents one user call.
\begin{itemize}[leftmargin=*]
  \item arrival time
  \item prompt\_tokens
  \item target\_gen\_tokens (or distribution)
  \item streaming flag
  \item priority / class (optional)
\end{itemize}

\subsubsection*{GPU / Worker}
A resource model:
\begin{itemize}[leftmargin=*]
  \item VRAM budget (GB)
  \item max concurrent sequences
  \item token throughput model (tokens/sec) for:
  \begin{itemize}[leftmargin=*]
    \item prefill
    \item decode
  \end{itemize}
\end{itemize}

\subsubsection*{KV cache manager}
A software memory allocator abstraction:
\begin{itemize}[leftmargin=*]
  \item allocates KV memory as tokens are added
  \item enforces VRAM limits
  \item triggers actions (queue/reject/evict/swap) when memory is tight
\end{itemize}

\subsection*{Simulation time model (V1 choice)}
V1 will use a \textbf{discrete-event simulation} (recommended) so the simulator advances directly from event to event (arrival, start\_prefill, start\_decode, finish, memory-pressure actions), rather than stepping at a fixed tick. This avoids accuracy issues from coarse time steps and keeps runs efficient.

(Alternative time-step mode can be added later if desired, but V1 is discrete-event.)

\section*{3) Key model assumptions (explicit)}
To keep the simulator realistic but tractable, the following assumptions are explicit and configurable:
\begin{itemize}[leftmargin=*]
  \item \textbf{Prefill vs decode service:} Prefill time scales with prompt\_tokens using a configurable prefill throughput; decode time scales with generated tokens using a configurable decode throughput.
  \item \textbf{Throughput sharing:} When multiple sequences are active, effective throughput is shared according to the chosen scheduling/batching rules (the simulator will not emulate kernel-level behavior, only a resource/latency model).
  \item \textbf{Batching abstraction:} Batching is modeled as an improvement in \emph{effective} throughput up to a cap (e.g., max batch tokens / max sequences), potentially with diminishing returns. The simulator does not attempt to reproduce exact transformer kernel timing.
  \item \textbf{KV memory growth:} KV memory grows approximately linearly with total cached tokens (prompt + generated) per active request, with a configurable ``bytes per token'' model (parameterized by layers/heads/hidden if desired).
\end{itemize}

\section*{4) What the inputs look like}

% You'll support two input modes:

\subsection*{A) Replay mode (recommended for reproducibility)}
A workload trace file (workload.jsonl or workload.parquet) with rows like:
\begin{verbatim}
request_id, arrival_time_ms, prompt_tokens, gen_tokens, streaming, priority
\end{verbatim}
This makes experiments deterministic and easy to debug.

% \subsection*{B) Synthetic workload mode (for quick scenarios)}
% Config includes distributions:
% \begin{itemize}[leftmargin=*]
%   \item arrival process: Poisson / bursty
%   \item prompt length distribution
%   \item gen length distribution
%   \item mixture distributions (e.g., 90\% short, 10\% long)
% \end{itemize}

\section*{5) What you're trying to compare/see}

You will run sweeps over three axes:

\subsection*{Axis 1: Policies}

\subsubsection*{Admission control}
\begin{itemize}[leftmargin=*]
  \item Admit if prompt fits now
  \item Admit if prompt + \textbf{decode KV reservation budget} fits (safer)
  \item Queue up to N; reject overflow
\end{itemize}
Note: ``decode KV reservation'' is intentionally a deterministic heuristic (e.g., reserve up to a configured cap, or reserve to a chosen quantile of gen length), not a learned forecast.

\subsubsection*{Scheduling / batching}
\begin{itemize}[leftmargin=*]
  \item FIFO
  \item shortest-first
  \item priority-based (premium/streaming)
  \item max batch tokens / max sequences
  \item prefill/decode mixing rule (important)
\end{itemize}

\subsubsection*{Memory pressure response}
\begin{itemize}[leftmargin=*]
  \item reject-on-pressure
  \item evict policy (LRU/FIFO/longest-first/lowest-priority)
  \item optional ``swap KV to CPU'' with latency penalty (v2 feature)
\end{itemize}

\subsection*{Axis 2: Hardware constraints}
\begin{itemize}[leftmargin=*]
  \item VRAM budget (e.g., 24GB / 40GB / 80GB)
  \item max concurrent sequences
  \item prefill/decode throughput assumptions
\end{itemize}

\subsection*{Axis 3: Workload characteristics}
\begin{itemize}[leftmargin=*]
  \item short-chat heavy vs long-context heavy
  \item steady vs bursty (arrival time)
  \item streaming \% vs non-streaming
  \item high arrival rate overload tests
\end{itemize}

\section*{6) Outputs (what you produce)}

Every run writes a self-contained run folder:
\begin{verbatim}
runs/<run_id>/
\end{verbatim}

\subsection*{Required outputs}

\subsubsection*{1) summary.json}
Top-line metrics:
\begin{itemize}[leftmargin=*]
  \item throughput (tokens/sec)
  \item completion rate
  \item reject rate
  \item OOM count (should be 0 in good configs)
  \item p50/p95/p99 end-to-end latency
  \item p50/p95 time-to-first-token (TTFT) if streaming modeled
  \item utilization stats (avg VRAM used, GPU busy time)
\end{itemize}

\subsubsection*{2) timeseries.csv (or parquet)}
Sampled every fixed simulated interval:
\begin{itemize}[leftmargin=*]
  \item time
  \item vram\_used
  \item active\_prefill
  \item active\_decode
  \item queue\_depth
  \item tokens\_generated\_delta
  \item rejects\_delta / evictions\_delta
\end{itemize}

\subsubsection*{3) events.jsonl (kept coarse)}
Events such as:
\begin{itemize}[leftmargin=*]
  \item arrival, enqueue, admit, start\_prefill, start\_decode, finish, reject, evict, oom
\end{itemize}
Avoid per-token events; use counters in timeseries (to prevent IO/file-size blowups).

\subsubsection*{4) run\_meta.json}
\begin{itemize}[leftmargin=*]
  \item git commit hash
  \item config hash
  \item seed
  \item timestamp
  \item scenario name
\end{itemize}

\section*{7) Validation and sanity checks (high-signal)}
Each sweep should include simple checks that validate the simulator behaves as expected:
\begin{itemize}[leftmargin=*]
  \item \textbf{Low load regime:} with low arrival rate, queue depth $\approx 0$ and latency $\approx$ modeled service time.
  \item \textbf{Saturation curve:} as arrival rate increases, utilization approaches 1 and tail latency (p95/p99) rises sharply after saturation.
  \item \textbf{VRAM monotonicity:} increasing VRAM should not worsen reject/OOM rates in otherwise identical settings.
  \item \textbf{Long-context tail:} introducing a small fraction of long-context requests disproportionately increases tail latency and/or reject rates.
  \item \textbf{No-OOM configs:} for ``good'' configs (with sufficient reservation or pressure response), OOM count should remain 0.
\end{itemize}

\section*{8) Project split: C++ vs Python vs React}

\subsection*{C++: the engine (``truth'')}
Goal: fast, deterministic simulation + artifact generation.

\subsubsection*{C++ responsibilities}
\begin{itemize}[leftmargin=*]
  \item Simulation kernel
  \begin{itemize}[leftmargin=*]
    \item discrete-event loop (recommended V1 choice)
    \item deterministic RNG
  \end{itemize}
  \item Core domain state
  \begin{itemize}[leftmargin=*]
    \item request lifecycle state machine
    \item GPU resource model
    \item queue(s)
    \item KV cache manager
    \begin{itemize}[leftmargin=*]
      \item simple version: bytes accounting per request
      \item better version: block allocator (paged KV blocks) for realism
    \end{itemize}
  \end{itemize}
  \item Policies
  \begin{itemize}[leftmargin=*]
    \item admission logic (including decode KV reservation heuristic)
    \item scheduling selection
    \item eviction/preemption trigger logic
  \end{itemize}
  \item Outputs writer
  \begin{itemize}[leftmargin=*]
    \item summary JSON
    \item timeseries CSV/Parquet
    \item events JSONL
  \end{itemize}
\end{itemize}

\subsubsection*{C++ non-goals}
\begin{itemize}[leftmargin=*]
  \item sweeping parameters
  \item plotting
  \item report writing
\end{itemize}

\subsection*{Python: experiments + analysis (``scientist'')}
Goal: run lots of scenarios and produce clean plots/reports.

\subsubsection*{Python responsibilities}
\begin{itemize}[leftmargin=*]
  \item Experiment runner
  \begin{itemize}[leftmargin=*]
    \item select traces, generate configs, launch runs
    \item launch many runs (subprocess)
    \item store artifacts in a sweep folder
  \end{itemize}
  \item Parsing + recomputing metrics
  \begin{itemize}[leftmargin=*]
    \item sanity-check metrics from raw events/timeseries
    \item compute extra metrics:
    \begin{itemize}[leftmargin=*]
      \item tail latency decomposition
      \item fairness (slowdown ratio per class)
      \item overload curves (latency vs arrival rate)
    \end{itemize}
  \end{itemize}
  \item Plotting
  \begin{itemize}[leftmargin=*]
    \item latency vs VRAM
    \item reject rate vs arrival rate
    \item VRAM usage time series
    \item throughput/latency tradeoff curves
  \end{itemize}
  \item Report generation
  \begin{itemize}[leftmargin=*]
    \item auto-generate a markdown report per sweep:
    \begin{itemize}[leftmargin=*]
      \item key plots
      \item table of results
      \item conclusions
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection*{React: minimal visualization (``demo layer'', optional)}
Goal: a simple single-page UI to view one run's results.

\subsubsection*{React responsibilities (keep it tiny)}
\begin{itemize}[leftmargin=*]
  \item Load summary.json + timeseries.csv (or a converted timeseries.json)
  \item Show:
  \begin{itemize}[leftmargin=*]
    \item KPI cards (p50/p95 latency, throughput, reject rate, avg VRAM)
    \item 2--3 plots:
    \begin{itemize}[leftmargin=*]
      \item VRAM used vs time
      \item queue depth vs time
      \item tokens/sec vs time (or throughput)
    \end{itemize}
    \item Dropdown to pick a run folder
  \end{itemize}
\end{itemize}

\subsubsection*{Implementation suggestion}
Put a tiny Node/Express static file server OR just use vite dev server and point it at a local runs/ directory via a simple file picker upload.
Use a lightweight chart lib (Recharts) or Plotly JS.
Don't build a full web backend. Static is enough.

\section*{9) Repo structure (clean + recruiter-friendly)}
\begin{verbatim}
kv-sim/
  cpp/
    include/
    src/
    CMakeLists.txt
    sim_main.cpp
  python/
    kvsim/                 # python package (imports used by scripts)
      __init__.py
      io.py                # read summary/timeseries/events
      metrics.py           # recompute/check metrics
      plots.py             # plotting utilities
      report.py            # report utilities
      sweep.py             # sweep orchestration helpers
    requirements.txt
  scripts/
    kvsim_sweep.py         # single entrypoint: run sweep + analyze + plots + report
    kvsim_run.py           # run one config (thin wrapper over C++ binary)
  web/                     # optional demo viewer
    src/
    package.json
  configs/
    examples/
  data/
    traces/                # replay traces (workload.jsonl / parquet)
  runs/                    # generated (gitignored)
  figures/                 # generated (gitignored)
  reports/                 # generated (gitignored)
  docs/
    model_assumptions.md
    policies.md
  README.md
\end{verbatim}

\section*{10) Minimal V1 you can ship fast (high signal)}

\subsection*{V1 features (Replay-only)}
\begin{itemize}[leftmargin=*]
  \item 1 GPU
  \item admission policy: (a) naive, (b) safe-by-\textbf{KV reservation budget}
  \item scheduling: FIFO
  \item KV cache: bytes accounting (no blocks yet)
  \item workload: \textbf{replay trace only} (deterministic)
  \item outputs: summary + timeseries + coarse events
  \item python: \textbf{single sweep entrypoint} + 3 plots + report
  \item react: load a run and display plots (optional)
\end{itemize}
This is already impressive if the doc + plots are crisp.

% \section*{11) V2 features (if you want to go deeper)}
% \begin{itemize}[leftmargin=*]
%   \item block/paged KV allocator (fragmentation + reuse)
%   \item prefill/decode mixing policy
%   \item swap-to-CPU (latency penalty model)
%   \item multi-GPU worker pool with simple load balancing (least-loaded), not model selection
%   \item synthetic workload generator + trace exporter
% \end{itemize}

\section*{12) How to run (local workflow)}

\subsection*{Build the C++ simulator}
From repo root:
\begin{verbatim}
mkdir -p cpp/build
cd cpp/build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . -j
\end{verbatim}
This should produce a simulator binary (example name: \texttt{kv\_sim}).

\subsection*{Run a single simulation (replay trace)}
Example: run from repo root with an example config:
\begin{verbatim}
./cpp/build/kv_sim \
  --config configs/examples/basic_fifo.json \
  --trace data/traces/basic_trace.jsonl \
  --out runs/basic_fifo_run
\end{verbatim}
Expected outputs in:
\begin{verbatim}
runs/basic_fifo_run/summary.json
runs/basic_fifo_run/timeseries.csv
runs/basic_fifo_run/events.jsonl
runs/basic_fifo_run/run_meta.json
\end{verbatim}

\subsection*{Run a parameter sweep (Python, replay-only)}
Create a virtual environment and install requirements (example):
\begin{verbatim}
python -m venv .venv
source .venv/bin/activate
pip install -r python/requirements.txt
\end{verbatim}

Run a sweep (example). Two replay-only overload options are supported:

\textbf{Option A: multiple traces} (recommended for clean apples-to-apples comparisons):
\begin{verbatim}
python scripts/kvsim_sweep.py \
  --scenario long_context_tail \
  --trace_list data/traces/trace_low.jsonl data/traces/trace_med.jsonl data/traces/trace_high.jsonl \
  --vram_list 24 40 80 \
  --policy_list fifo reserve_budget \
  --out_dir runs/sweeps/2026_01_01_longtail \
  --make_plots --make_report
\end{verbatim}

\textbf{Option B: time-warp a single trace} (deterministic arrival-rate scaling):
\begin{verbatim}
python scripts/kvsim_sweep.py \
  --scenario long_context_tail \
  --trace data/traces/trace_base.jsonl \
  --time_scale_list 0.8 1.0 1.25 \
  --vram_list 24 40 80 \
  --policy_list fifo reserve_budget \
  --out_dir runs/sweeps/2026_01_01_longtail \
  --make_plots --make_report
\end{verbatim}

\subsection*{Run the minimal web viewer (React, optional)}
From \texttt{web/}:
\begin{verbatim}
cd web
npm install
npm run dev
\end{verbatim}
\end{document}

\end{document}
